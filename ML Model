import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your dataframe with the data already loaded
# Example: df = pd.read_csv('your_data.csv')

# Define feature columns (all columns except the target)
# Adjust the target variable based on what you want to predict
# Common options: cumulative_gpa, interview_score, or previous_year_gpa

# Option 1: Predict cumulative_gpa
feature_cols = ['attendance_rate', 'penalty_points', 'important_absences', 
                'previous_year_gpa', 'failed_credits', 'awards', 'publications',
                'leadership_roles', 'fees_paid', 'has_serious_violations', 
                'interview_score', 'semester', 'year']
target_col = 'cumulative_gpa'

# Split the data into features (X) and target variable (y)
X = df[feature_cols]
y = df[target_col]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create a linear regression model
model = LinearRegression()

# Train the model using the training sets
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Evaluate the model
print('Model Evaluation Metrics:')
print('=' * 50)
print(f'Mean Absolute Error: {metrics.mean_absolute_error(y_test, y_pred):.4f}')
print(f'Mean Squared Error: {metrics.mean_squared_error(y_test, y_pred):.4f}')
print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, y_pred)):.4f}')
print(f'RÂ² Score: {metrics.r2_score(y_test, y_pred):.4f}')

# Feature importance (coefficients)
print('\n' + '=' * 50)
print('Feature Coefficients (Importance):')
print('=' * 50)
feature_importance = pd.DataFrame({
    'Feature': feature_cols,
    'Coefficient': model.coef_
}).sort_values('Coefficient', ascending=False)
print(feature_importance)

# Visualization 1: Actual vs Predicted
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue', edgecolors='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title(f'Actual vs Predicted {target_col}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Visualization 2: Residual plot
plt.figure(figsize=(10, 6))
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6, color='purple', edgecolors='k')
plt.axhline(y=0, color='r', linestyle='--', lw=2)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Visualization 3: Feature importance bar chart
plt.figure(figsize=(12, 6))
colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient']]
plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, alpha=0.7)
plt.xlabel('Coefficient Value')
plt.ylabel('Features')
plt.title('Feature Importance (Regression Coefficients)')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

# Visualization 4: Top feature correlation with target
top_feature = feature_importance.iloc[0]['Feature']
plt.figure(figsize=(10, 6))
plt.scatter(df[top_feature], df[target_col], alpha=0.6, color='green', edgecolors='k', label='Actual Data')
# Sort values for proper line plotting
sorted_indices = df[top_feature].argsort()
plt.plot(df[top_feature].iloc[sorted_indices], 
         model.predict(X)[[i for i in sorted_indices]], 
         color='blue', linewidth=2, label='Regression Prediction')
plt.xlabel(top_feature)
plt.ylabel(target_col)
plt.title(f'Most Important Feature: {top_feature} vs {target_col}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print('\n' + '=' * 50)
print(f'Model trained successfully!')
print(f'Predicting: {target_col}')
print(f'Using {len(feature_cols)} features')
print('=' * 50)